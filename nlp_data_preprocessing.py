# -*- coding: utf-8 -*-
"""NLP Data Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lFPpX0hDSw_c3QSWiX_IDMq3wq2ZZp-a
"""

#!pip install nltk
#!pip install spacy

import pandas as pd
import nltk
import spacy

nltk.download("all")

"""https://drive.google.com/file/d/1CT81bIgzIg3yCfuFPmUV-WAOuwwfN_-A/view?usp=share_link"""

url = 'https://drive.google.com/file/d/1hZo19J-wTXSFxYX8KO3uFdPpdskd83mW/view?usp=share_link'
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
twitter_data = pd.read_csv(path)

twitter_data.sample(10)

"""Lower Case conversion"""

twitter_data["tweet_lowcase"]=twitter_data["text"].apply(lambda x:str(x).lower())
twitter_data.sample(10)

"""Tokenizing"""

from nltk.tokenize import word_tokenize 
twitter_data["word_tokens"] = twitter_data["tweet_lowcase"].apply(lambda x:word_tokenize(str(x)))
#lambda function to apply on all rows
#str() function to avoid numeric and other errors
twitter_data[["text","word_tokens"]].sample(10)

"""Expanding shortforms"""

contra_Expan_Dict = {"ain`t": "am not","aren`t": "are not","can`t": "cannot","can`t`ve": "cannot have","`cause": "because",
"could`ve": "could have","couldn`t": "could not","couldn`t`ve": "could not have","didn`t": "did not",
"doesn`t": "does not","don`t": "do not","hadn`t": "had not","hadn`t`ve": "had not have","hasn`t": "has not",
"haven`t": "have not","he`d": "he would","he`d`ve": "he would have","he`ll": "he will","he`ll`ve": "he will have",
"he`s": "he is","how`d": "how did","how`d`y": "how do you","how`ll": "how will",
"how`s": "how does","i`d": "i would","i`d`ve": "i would have","i`ll": "i will","i`ll`ve": "i will have","i`m": "i am",
"i`ve": "i have","isn`t": "is not","it`d": "it would","it`d`ve": "it would have","it`ll": "it will","it`ll`ve": "it will have",
"it`s": "it is","its":"it is","im":"i am","cant":"can not", "let`s": "let us","ma`am": "madam","mayn`t": "may not","might`ve": "might have","mightn`t": "might not",
"mightn`t`ve": "might not have","must`ve": "must have","mustn`t": "must not","mustn`t`ve": "must not have","needn`t": "need not","needn`t`ve": "need not have",
"o`clock": "of the clock","oughtn`t": "ought not","oughtn`t`ve": "ought not have","shan`t": "shall not",
"sha`n`t": "shall not","shan`t`ve": "shall not have","she`d": "she would",
"she`d`ve": "she would have","she`ll": "she will","she`ll`ve": "she will have", "btw":"by the way",
"she`s": "she is","should`ve": "should have","shouldn`t": "should not","shouldn`t`ve": "should not have","so`ve": "so have","so`s": "so is",
"that`d": "that would","that`d`ve": "that would have","that`s": "that is","there`d": "there would","there`d`ve": "there would have","there`s": "there is",
"they`d": "they would","they`d`ve": "they would have","they`ll": "they will","they`ll`ve": "they will have","they`re": "they are","they`ve": "they have",
"to`ve": "to have","wasn`t": "was not"," u ": " you "," ur ": " your "," n ": " and ","won`t": "would not", "wont": "would not",
"dis": "this","bak": "back","brng": "bring" ,"d": "the", "n": "and", "u": "you"}

def expanded_form(x):
  if x in contra_Expan_Dict.keys():
    return(contra_Expan_Dict[x])
  else:
    return(x)

x=str(twitter_data["tweet_lowcase"][6207])
print("original tweet ==>", x)
x=x.split()
print("Expanded form ==>",[expanded_form(t) for t in x])

twitter_data["tweet_expanded"]=twitter_data["tweet_lowcase"].apply(lambda x:[expanded_form(t) for t in str(x).split()])
twitter_data[["text","tweet_expanded"]].sample(10)

"""Stopwords removal"""

#NLTK 179 stopwords
from nltk.corpus import stopwords 
nltk_stop_words = set(stopwords.words('english')) ##Selecting the stop words from the Language
print("Number of Stop words in NLTK =", len(nltk_stop_words))
print(sorted(nltk_stop_words))

#SPACY 326 stopwords
from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords
print("Number of Stop words in spaCy =", len(spacy_stopwords))
print(sorted(spacy_stopwords))
#Spacy stopwords list looks better.

x=twitter_data["tweet_expanded"][16355]
print("original tweet ==>", x)
print("After Removing Stopwords ==>",[t for t in x if t not in spacy_stopwords])

twitter_data["After_Removing_Stopwords"] = twitter_data["tweet_expanded"].apply(lambda x:[t for t in x if t not in spacy_stopwords ])
twitter_data[["text","After_Removing_Stopwords"]].sample(10)

"""Add Custom Stowords"""

from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords
spacy_stopwords.update({"would", "rt","like", "ha", "lol", "need", "do"})
print("New Number of Stop words in spaCy =", len(spacy_stopwords))
print(sorted(spacy_stopwords))

twitter_data["After_Removing_Stopwords"] = twitter_data["tweet_expanded"].apply(lambda x:[t for t in x if t not in spacy_stopwords ])
twitter_data[["text","After_Removing_Stopwords"]].sample(10)

"""Regularize expression . """

import re
def clean_with_re(x):
  x=str(x)
  x=re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'," ", x) #Remove URLs
  x=re.sub(r'[^\w ]+', "", x) # Remove Punctuation-1
  x=re.sub(r"[,!@&\'?\.$%_]"," ", x) # Remove Punctuation-2
  x=re.sub(r"\d+"," ", x) #Remove digits
  return(x)

twitter_data["tweet_cleaned_Regex"]=twitter_data["After_Removing_Stopwords"].apply(lambda x:clean_with_re(x))
twitter_data[["text","tweet_cleaned_Regex"]].sample(10)

"""Spelling Correction"""

#!pip install textblob
#!python textblob.download_corpora

from textblob import TextBlob
sample_tweet="What an grat and amazimg week. I am excited to learn data scienec"
corrected_tweet=TextBlob(sample_tweet).correct()
corrected_tweet

"""since spelling correction is distorting the document, we avoid it for keeping information intact.

Stemming / lemmatization
"""

spacy_model = spacy.load('en_core_web_sm')

sample_tweet=twitter_data["tweet_cleaned_Regex"][0]
print("Original Text =", sample_tweet)
print("Lemmatization Results =", " ".join([t.lemma_ for t in spacy_model(str(sample_tweet))]))
#print("Lemmatization PRON removed =", " ".join([t.lemma_ for t in spacy_model(str(sample_tweet)) if t.lemma_ !="-PRON-" ]))

twitter_data["Lemmatized_tweet"] = twitter_data["tweet_cleaned_Regex"].apply(lambda x:" ".join([t.lemma_ for t in spacy_model(str(x))if t.lemma_ !="-PRON-" ]))
twitter_data[["text","Lemmatized_tweet"]].sample(10)

"""final cleaning"""

spacy_stopwords.update({"would", "rt","like", "ha", "lol", "need", "do", "oh", "haha"})
twitter_data["Final_Cleaned_Tweet"] = twitter_data["Lemmatized_tweet"].apply(lambda x:[t for t in str(x).split() if t not in spacy_stopwords ])
twitter_data["Final_Cleaned_Tweet_tokens"]=twitter_data["Final_Cleaned_Tweet"].apply(lambda x: " ".join(x) )
twitter_data[["text","Final_Cleaned_Tweet_tokens"]].sample(10)

"""word cloud"""

# Commented out IPython magic to ensure Python compatibility.
#!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

final_text="".join(twitter_data["Final_Cleaned_Tweet_tokens"])
len(final_text)

plt.figure(figsize = (15, 15), facecolor = None) 
wc=WordCloud(colormap='Set2').generate(final_text)
plt.imshow(wc)
plt.axis("off")
plt.show()

"""A Single function for pre-processing"""

def pre_processing(input_data, text_col):
  input_data["text_col_clean"]=input_data[text_col].apply(lambda x:str(x).lower())
  input_data["text_col_clean"]=input_data["text_col_clean"].apply(lambda x:[expanded_form(t) for t in str(x).split()])
  input_data["text_col_clean"]=input_data["text_col_clean"].apply(lambda x:[t for t in x if t not in spacy_stopwords ])
  input_data["text_col_clean"]=input_data["text_col_clean"].apply(lambda x:clean_with_re(x))
  input_data["text_col_clean"]=input_data["text_col_clean"].apply(lambda x:" ".join([t.lemma_ for t in spacy_model(str(x))if t.lemma_ !="-PRON-" ]))
  input_data["text_col_clean"]=input_data["text_col_clean"].apply(lambda x:[t for t in str(x).split() if t not in spacy_stopwords ])
  input_data["text_col_clean"]=input_data["text_col_clean"].apply(lambda x: " ".join(x) )
  print(input_data[[text_col,"text_col_clean"]])

pre_processing(input_data=twitter_data, text_col="text")

"""Document Term Matrix"""

from sklearn.feature_extraction.text import CountVectorizer

countvec1 = CountVectorizer(min_df= 5) #minimum word freq=5
dtm_v1 = pd.DataFrame(countvec1.fit_transform(twitter_data['Final_Cleaned_Tweet_tokens']).toarray(), columns=countvec1.get_feature_names(), index=None)
print(dtm_v1.shape)
dtm_v1